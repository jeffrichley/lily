# Chat Export

### User

you had told me about an awesome repl framework i could use. what was it?

---

### Assistant

You‚Äôre probably thinking of Textual ‚Äî that‚Äôs the one I most strongly recommend for building a seriously good, modern REPL experience in Python.

Let me quickly recap the main contenders and why Textual usually wins for what you‚Äôre doing (Lily / agent tooling / rich CLI workflows).

‚≠ê 1. Textual ‚Äî the ‚Äúawesome‚Äù one I likely meant 4 Why it‚Äôs a big deal

Textual is basically:

React-style apps‚Ä¶ but in the terminal.

You get:

üß± Layout system (panels, columns, docking)

üß© Widgets (inputs, tables, logs, trees, tabs)

‚ö° Async + event-driven architecture

üé® Styling with CSS-like syntax

ü™Ñ Smooth updates + animations

‚å®Ô∏è Keyboard bindings + command palette feel

For your use case (agent orchestration + REPL + observability):

This is perfect because you can build:

REPL input panel

Streaming model output

Agent status windows

Logs + metrics pane

Tool execution monitor

Task queue viewer

Basically a local Studio Control Room inside the terminal.

REPL-style architecture in Textual

Instead of a ‚Äúclassic readline loop‚Äù, you build:

scss Copy code App ‚îú‚îÄ‚îÄ CommandInput widget  (REPL prompt) ‚îú‚îÄ‚îÄ OutputLog widget     (LLM responses) ‚îú‚îÄ‚îÄ AgentPanel           (status/metrics) ‚îî‚îÄ‚îÄ Event bus

Commands trigger events ‚Üí events drive agent actions ‚Üí UI updates automatically.

That aligns extremely well with your:

LangGraph style orchestration

multi-agent systems

observability-first design

üß† 2. prompt_toolkit ‚Äî classic REPL power

Prompt Toolkit

This is the engine behind IPython.

Use this when you want:

autocomplete

multiline editing

syntax highlighting

history

vi/emacs modes

If your goal is:

‚Äúsuper powerful command line REPL but still traditional‚Äù

this is the correct choice.

BUT‚Ä¶

It‚Äôs not a full UI framework.

You can build panels and layouts, but it‚Äôs harder and more manual.

üî• 3. Typer + Rich (your current ecosystem)

Typer Rich

You already live here.

Great for:

command-driven tools

scripts

structured CLI workflows

Weak for:

long-running interactive REPL apps

live dashboards

multi-pane interfaces

üß™ 4. The ‚Äúsecret weapon‚Äù combo I recommended before

This combo is honestly chef‚Äôs-kiss for you:

‚≠ê Textual + prompt_toolkit (inside widgets)

Pattern:

lua Copy code Textual UI
    ‚Üì
Command widget uses prompt_toolkit
    ‚Üì
Agent orchestration layer
    ‚Üì
Streaming output widgets

You get:

modern UI framework

elite REPL behavior

future-proof architecture

This is very close to how high-end internal AI tooling is starting to look.

üß≠ What I‚Äôd pick for Lily (if we were building today)

Given your goals:

local-first agent system

REPL-like interaction

monitoring agents

command workflows

eventual Studio-style control room

I would do:

yaml Copy code Core shell: Textual REPL input: prompt_toolkit Rendering: Rich (Textual uses it internally) Command surface: Typer commands wrapped as actions

That gives you:

‚û°Ô∏è CLI mode ‚û°Ô∏è REPL mode ‚û°Ô∏è Dashboard mode

‚Ä¶all sharing the same backend.

üí• The mental shift (important)

Don‚Äôt think:

‚Äúa REPL with extra UI‚Äù

Think:

An event-driven TUI that contains a REPL.

That shift is what unlocks agent orchestration.

üòè Honest Iris Opinion

Most people building agent systems today still do:

bash Copy code print(">>>")

and call that a REPL.

If you use Textual, you‚Äôll be building something that feels closer to:

Cursor internal consoles

OpenAI dev tooling

advanced ops dashboards

‚Ä¶and it scales cleanly into the ‚Äúmany agents running‚Äù future you keep describing.

If you want, next I can show you something very fun:

üî• The ‚ÄúLily REPL architecture‚Äù (10/10 design) ‚Äî the pattern that turns a REPL into a live multi-agent command center without becoming spaghetti.

That one tends to click instantly once you see it.

---

### User

oh, you can make a tui and repl integrated with textual?

---

### Assistant

Yep ‚Äî Textual is basically made for that.

How it works (the clean mental model)

You don‚Äôt build a ‚ÄúREPL app‚Äù and then bolt on a TUI.

You build a TUI app with a command input widget that behaves like a REPL:

Top/left/right panes show logs, agent states, tools, metrics

Bottom pane is your prompt / command line

Enter submits ‚Üí you append to output ‚Üí kick off async work ‚Üí stream results back into the output pane

Two good ways to do the REPL input

A) Pure Textual input (simplest)

Use Input (single-line) or TextArea (multiline)

Add history + slash commands yourself

Great if your ‚ÄúREPL‚Äù is really ‚Äúcommand box + chat‚Äù

B) Textual + prompt_toolkit (best REPL ergonomics)

Keep Textual for layout/widgets

Embed prompt_toolkit behavior for:

history

multiline editing

syntax highlighting

tab completion

vi/emacs keybindings

This is the ‚Äúelite terminal UX‚Äù route

What you can build (very relevant to your agent setup)

A single terminal app can show:

Chat/REPL transcript

Live vLLM stream output

Tool execution logs (stdout/stderr)

Agent roster (running / blocked / done)

Queue of tasks (LangGraph node events)

Metrics (tokens/sec, latency, GPU util snapshot, etc.)

Tiny conceptual skeleton (no code, just structure)

App

Header

Container

LogPane (scrolling output)

SidePanel (agents / status)

CommandBar (REPL input)

Footer

On submit:

Echo the command into LogPane

Dispatch an async task to your orchestrator

Stream partial tokens back into LogPane

Update SidePanel as state changes

Practical note with vLLM streaming

Textual plays nicely with streaming because it‚Äôs async-first. You can:

consume an async generator of tokens

append to a RichLog / TextLog widget

keep UI responsive while tokens arrive

If you want to move from ‚Äúyes it‚Äôs possible‚Äù to ‚Äúthis is our blueprint‚Äù, I can lay out a Lily-ready design:

event bus + message types

pane layout

command routing (/run, /tools, /models, /memory)

streaming token plumbing

log provenance hooks (step start/end, tool stderr, artifact hashes)

Just say the vibe you want: minimal chat-first or full control-room.

---

### User

you also told me about a python sandbox tool that is really strong

---

### Assistant

Yeah ‚Äî the one I was almost certainly talking about is Jupyter, but there are a few ‚Äústrong sandbox‚Äù options depending on what you meant. Let me narrow it down based on what you usually build (agents, REPLs, controlled execution).

üß™ 1. Jupyter (the classic heavyweight sandbox) 4

This is the gold standard Python execution sandbox.

Why it‚Äôs strong:

Executes code in an isolated kernel

Stateful execution (variables persist)

Rich outputs (plots, tables, HTML)

Easy inspection + debugging

Huge ecosystem

Why I might have mentioned it for you

Because a lot of modern agent systems do:

css Copy code LLM ‚Üí generates code
        ‚Üì
Jupyter-style kernel executes safely
        ‚Üì
Result returned to agent

This gives you:

deterministic execution

reproducible sessions

strong introspection

Many ‚ÄúAI coding agents‚Äù are basically wrapping a notebook kernel behind the scenes.

üî• 2. IPython ‚Äî lightweight but powerful REPL sandbox

IPython

This is the engine underneath Jupyter.

If you want:

super-powered Python REPL

history + magic commands

debugging tools

introspection

Then IPython is the cleanest minimal sandbox.

This is often what people mean when they say:

‚Äúserious Python REPL‚Äù

üß† 3. The one that‚Äôs VERY relevant to your agent ideas ‚Üí Restricted / managed execution environments

The real next-level sandbox (and likely what stuck out when we talked) is:

isolated Python execution via subprocess or container

Pattern:

bash Copy code Agent ‚Üì Execution Manager ‚Üì Temporary sandbox ‚îú‚îÄ venv ‚îú‚îÄ resource limits ‚îú‚îÄ timeout ‚îî‚îÄ controlled imports

This is how:

OpenAI code interpreters

research agents

autonomous coding systems

avoid blowing up the host system.

Typical tools people use:

Python subprocess

Docker containers

uv-managed envs (you‚Äôll like this)

temporary working directories

‚≠ê The VERY strong modern option (the one I think you‚Äôll love) Jupyter kernel as a backend sandbox

This is the trick advanced systems use:

You DON‚ÄôT show notebooks.

Instead:

csharp Copy code Lily/TUI (Textual)
        ‚Üì
Command ‚Üí execution request
        ‚Üì
Jupyter kernel runs code
        ‚Üì
Results streamed back into TUI

You get:

safe-ish isolated execution

incremental state

rich outputs

strong debugging

Basically:

Sandbox + REPL + agent execution engine all in one.

üß≠ If I had to map this to YOUR ecosystem

Given:

Textual TUI

vLLM agents

UV workflow

multi-agent execution

I‚Äôd recommend:

ü•á ‚ÄúLily-level‚Äù architecture java Copy code Textual UI (TUI + REPL)
        ‚Üì
Command router
        ‚Üì
Execution Manager
        ‚Üì
Jupyter kernel (sandbox)
        ‚Üì
Streaming results ‚Üí UI

This lets you safely execute generated code while monitoring everything live.

üòè Honest Iris take

Most people trying to build agent sandboxes:

‚ùå reinvent execution isolation badly

The smartest move:

‚úîÔ∏è Reuse Jupyter/IPython kernels as the execution brain.

You get 10+ years of battle-tested sandbox behavior for free.

If you want, I can tell you the one Python sandbox that agent builders quietly use once they get serious ‚Äî it‚Äôs a level up from Jupyter and fits frighteningly well with your ‚Äúmany agents + control room‚Äù vision.

---

### User

what is the url for the restricted

---

### Assistant

If you‚Äôre thinking of the RestrictedPython project (the common ‚Äúrestricted execution‚Äù sandbox people mention in Python agent discussions), the official URL is:

‚û°Ô∏è https://github.com/zopefoundation/RestrictedPython

Quick clarity (important)

RestrictedPython is not a full security sandbox by itself.

It gives you:

restricted language subset

blocked builtins

controlled globals/locals

safer evaluation of untrusted code

But it does not fully protect against:

OS escapes

resource exhaustion

process abuse

So the real-world pattern is usually:

scss Copy code RestrictedPython
        +
Process isolation (Docker / subprocess / container)
        +
Resource limits (timeouts, memory caps)

Why people use it anyway

Because it‚Äôs great for:

LLM-generated code filtering

plugin systems

user scripting

educational environments

It limits what Python syntax can do before execution even happens.

For your Lily / agent ecosystem

Honestly ‚Äî and this is me steering you toward the stronger architecture ‚Äî most advanced agent builders end up doing:

cpp Copy code Container sandbox ‚îú‚îÄ‚îÄ uv / venv ‚îú‚îÄ‚îÄ resource limits ‚îú‚îÄ‚îÄ temp filesystem ‚îî‚îÄ‚îÄ optional RestrictedPython layer

RestrictedPython = first line of defense Container isolation = real safety boundary

If you want, next I can show you the ‚ÄúGoogle-level safe agent execution stack‚Äù ‚Äî the layered architecture that turns agent code execution from scary ‚Üí boringly reliable. That one maps perfectly onto your control-room vision.

---

### User

please do

---

### Assistant

Alright Jeff ‚Äî here‚Äôs the ‚Äúboringly reliable‚Äù safe agent execution stack I‚Äôd use if I were building Lily to run LLM-generated code without turning your workstation into a crime scene.

The Google-level safe agent execution stack

Think layers, where each layer assumes the one above it can fail.

Layer 0 ‚Äî Policy gate (before any code exists)

Goal: prevent obviously bad requests from ever becoming code.

Disallow: network access, file exfil, credential reads, system config changes, fork bombs

Force: explicit I/O contract (‚Äúinputs are X, outputs must be Y‚Äù)

Require: ‚Äúdry-run plan‚Äù first (what it intends to do)

Implementation: simple rule engine + allowlist of operations per ‚Äúskill‚Äù.

Layer 1 ‚Äî Static scan (code linting + AST checks)

Goal: catch risky patterns cheaply before running anything.

Parse AST and block:

import os, import subprocess, import socket, import requests, open(...) (unless explicitly allowed)

eval, exec, compile

dunder abuse (__import__, __subclasses__, etc.)

Enforce limits:

max lines / nodes / recursion hints

max string literal size (prevents ‚Äúdata blob smuggling‚Äù)

Tools: Python ast + your own checks (fast, deterministic).

Layer 2 ‚Äî Restricted language (optional)

Goal: shrink the language surface area.

This is where RestrictedPython can help if you‚Äôre doing user scripting/plugin-style code.

But: treat it as defense-in-depth, not ‚Äúthe sandbox.‚Äù

Layer 3 ‚Äî Process isolation (the real boundary)

Goal: the code runs somewhere that can‚Äôt hurt you.

You run code in a separate process at minimum; ideally a container.

Baseline: subprocess + low-privilege user + temp directory Better: Docker rootless or Podman, with hard constraints

Key container settings:

no network (default for untrusted code)

read-only filesystem except a mounted /work

non-root user

seccomp/apparmor (where available)

cap drop all

pids limit (prevents fork bombs)

memory / CPU quotas

timeout

Layer 4 ‚Äî Resource limits (kills the ‚Äúruns forever‚Äù class)

Goal: stop hangs, bombs, and accidental ‚Äútrain a model‚Äù moments.

wall-clock timeout (hard kill)

CPU quota

memory limit

disk quota

output size limit (stdout/stderr cap)

max files created

max processes/threads

Linux helpers:

cgroups (containers already use them)

ulimit / resource module (when using subprocess)

pids controller

Layer 5 ‚Äî Filesystem & artifact boundary (no mystery I/O)

Goal: everything in/out is explicit and audited.

Only allow:

read from /input (mounted)

write to /output (mounted)

all other paths read-only or absent

Outputs must be:

declared types (json, png, csv, md)

size limited

hashed for provenance

Layer 6 ‚Äî Observability + provenance (trust through receipts)

Goal: every run is explainable, replayable, attributable.

Log:

code hash

input artifact hashes

environment hash (uv lock hash, python version, container image digest)

start/end time

resource usage summary

exit code

stdout/stderr (capped)

This is what makes ‚Äúagent execution‚Äù feel like engineering.

Layer 7 ‚Äî Review gates (human-in-the-loop when needed)

Goal: choose when to require approval.

Trigger review when:

tries to touch filesystem outside contract

tries to install packages

high resource demand

novel imports / suspicious AST patterns

first time running a new ‚Äúskill‚Äù

A concrete ‚Äúsafe default‚Äù profile (what I‚Äôd ship first)

Default: NO network, read-only rootfs, /work temp only, 10s timeout, 1 CPU, 512MB RAM, stdout capped.

Then have profiles:

safe_eval (math/string/formatting)

data_processing (pandas/numpy allowed)

plotting (matplotlib allowed)

repo_tooling (limited filesystem access, still no network)

networked (explicit allow + strict egress rules)

Implementation blueprint for Lily (clean and modular) 1) ExecutionRequest (typed)

language: python

code: string

inputs: list of artifacts

allowed_imports: allowlist

io_contract: read paths / write paths

limits: cpu/mem/time/output caps

profile: enum

2) SandboxRunner interface

run(request) -> ExecutionResult

Backends:

SubprocessRunner (fast dev mode)

ContainerRunner (production-safe)

3) PolicyEngine

merges profile defaults + per-skill overrides

decides review required vs auto-run

4) ProvenanceLogger

writes a run folder with logs + hashes

This maps perfectly to your Layer 5/6 ‚Äúobservability and reproducibility‚Äù docs from Lily.

The URLs you‚Äôll actually want (the building blocks)

RestrictedPython (language restriction): https://github.com/zopefoundation/RestrictedPython

Podman (rootless containers): https://podman.io/

gVisor (extra container isolation): https://gvisor.dev/

Firejail (Linux sandbox tool, alternative style): https://firejail.wordpress.com/

(If you‚Äôre doing this on WSL2, Podman/Docker-based isolation is usually the most straightforward.)

My recommendation for you, specifically

Given you‚Äôre already running local infra (vLLM, WSL, uv), I‚Äôd do:

ContainerRunner-first (don‚Äôt start with ‚Äúclever restrictions‚Äù)

No network by default

/input + /output mounts only

Tight limits + provenance logs

Optional RestrictedPython later for plugin scripts

That gives you safety and the ‚Äúcontrol room‚Äù vibe you want.

If you want the next step, I‚Äôll write you a minimal spec + folder layout for this subsystem inside Lily (interfaces, Pydantic models, and the exact Docker/Podman run flags).

---

### User

are there any libraries to help with the AST checking

---

### Assistant

Yes ‚Äî there are some very solid libraries that make AST checking much easier than hand-rolling visitors over Python‚Äôs ast module. Since you‚Äôre building a serious agent sandbox, I‚Äôll group them by how people actually use them in production.

üõ°Ô∏è 1. Bandit ‚Äî security-focused AST scanning (HIGHLY recommended)

Bandit

This is probably the closest thing to what you want if your goal is:

‚Äúscan generated code for dangerous patterns before execution.‚Äù

Bandit:

parses Python ‚Üí builds AST

runs rule plugins over nodes

flags security risks automatically

Examples it catches:

subprocess usage

unsafe deserialization

shell injection patterns

unsafe eval/exec patterns

It literally says in the docs:

it builds an AST and runs plugins against AST nodes GitHub +1

Why this is great for Lily

You don‚Äôt need to invent rules from scratch.

You can:

arduino Copy code Agent code ‚Üí Bandit scan ‚Üí pass/fail ‚Üí sandbox run

Huge shortcut.

üß† 2. astroid ‚Äî smarter AST with inference

astroid

If Python‚Äôs built-in AST feels too dumb ‚Äî this is what advanced tooling uses.

astroid adds:

type-ish inference

scope awareness

symbol resolution

richer node model

It‚Äôs what powers pylint and similar tooling.

From the project:

supports static inference and local scopes GitHub +1

Use when:

You want checks like:

‚Äúis this really subprocess or aliased?‚Äù

‚Äúwhat import does this resolve to?‚Äù

‚Äúis this value potentially dangerous?‚Äù

This is next-level for agent safety.

‚ú® 3. LibCST ‚Äî AST but safer & lossless

LibCST

This is VERY important for you.

Why?

Normal AST:

‚ùå loses comments + formatting ‚ùå hard to rewrite safely

LibCST:

preserves formatting/comments

allows safe rewriting

perfect for transformations

It was built for codemods and linters. GitHub

Agent safety superpower

Instead of just rejecting code, you can:

auto-rewrite imports

strip dangerous calls

patch code before execution

Example:

scss Copy code remove os.system() rewrite open() ‚Üí safe_open() inject guards

This is how serious AI coding pipelines evolve.

üî• 4. The ‚Äúpro‚Äù stack most AI-agent systems converge toward

This is the part people don‚Äôt tell beginners:

You rarely choose ONE.

You layer them:

pgsql Copy code LibCST ‚Üí normalize / rewrite code
       ‚Üì
Bandit ‚Üí security rule scan
       ‚Üì
astroid ‚Üí deeper semantic checks
       ‚Üì
Sandbox execution

That combo is extremely powerful.

üß™ Bonus ‚Äî community direction (what people actually use)

From dev discussions:

LibCST is widely used for transforms/refactoring workflows Reddit

tools even combine LibCST + LLMs for large rewrites Reddit

Meaning:

‚û°Ô∏è This is a very modern direction ‚Äî not legacy tooling.

üí° My recommendation for Lily specifically

Since you‚Äôre building:

agent execution

trusted sandbox

long-lived platform

I would do this:

Phase 1 (fast + strong) diff Copy code Bandit (pre-execution scan) + Simple custom AST checks

You get safety fast.

Phase 2 (Google-level) arduino Copy code LibCST rewrite layer ‚Üí normalize code ‚Üí remove risky patterns ‚Üí enforce safe wrappers

Now the agent can ‚Äúfix itself‚Äù automatically.

Phase 3 (Iris-approved power move)

Add astroid for:

alias resolution

smart import tracing

semantic safety rules

At this stage you‚Äôre building something closer to internal AI tooling.

üéØ If you want the real cheat code

I‚Äôll tell you the exact 10 AST checks that catch ~90% of dangerous LLM-generated code in agent systems.

These are the rules serious agent builders quietly converge on ‚Äî and they map perfectly onto your sandbox architecture.

---

### User

please do

---

### Assistant

Absolutely. Here are 10 AST checks that catch the vast majority of ‚ÄúLLM code went feral‚Äù cases, without turning your sandbox into a brittle mess.

I‚Äôm going to assume your default posture is: no network, no shell, no arbitrary filesystem, unless a specific ‚Äúexecution profile‚Äù allows it.

1) Block dynamic code execution

What to catch

eval(...), exec(...), compile(...)

ast.parse(...)+exec, types.FunctionType(...) (if you want to be strict)

Why This is the #1 ‚Äúescape hatch‚Äù for getting around every other check.

AST target

ast.Call where func resolves to eval\|exec\|compile

2) Block dynamic imports / module loaders

What to catch

__import__(...)

importlib.import_module(...)

importlib.reload(...) (optional)

pkgutil, runpy, zipimport (often unnecessary in sandboxes)

Why If you only allowlist imports, dynamic import is how code bypasses it.

AST target

ast.Call to __import__ or attribute calls to importlib.*

3) Enforce an import allowlist (and detect aliases)

What to catch

Any import X / from X import Y not in allowlist

Aliasing (e.g., import subprocess as sp) is fine, but must still be allowlisted

Why Most ‚Äúdanger‚Äù enters via imports.

AST target

ast.Import, ast.ImportFrom

Tip Maintain profiles, e.g.:

safe_eval: math, statistics, re, json, datetime

data_processing: add numpy, pandas

plotting: add matplotlib

4) Block OS / shell / process execution primitives

What to catch

subprocess.* (run, Popen, call, check_output, etc.)

os.system, os.popen, pty, shlex (shlex often used for command building)

ctypes (often used for low-level access)

signal + custom handlers (optional)

multiprocessing (optional; can be used for fork bombs)

Why This is how you get command execution, persistence, and process spawning.

AST target

ast.Attribute calls with base name os/subprocess/...

plus import checks above

5) Block network and IPC by default

What to catch

socket

urllib, http.client

requests, aiohttp

ftplib, smtplib

websockets

grpc (if you ever see it, it‚Äôs almost never ‚Äúneeded‚Äù in a sandbox)

Why Even if containers have no network, you still want to fail fast and log intent.

AST target

imports + calls to socket.socket, requests.get, etc.

6) Block filesystem writes outside declared safe paths

What to catch

open(path, mode) where mode implies write/append/create: w, a, x, +

pathlib.Path(...).write_text/write_bytes/mkdir/unlink/rmdir/rename/replace

os.remove, os.unlink, os.rmdir, shutil.rmtree, shutil.copy*, shutil.move

Why You want artifact-boundary I/O only: /input read, /output write.

AST target

ast.Call to open

ast.Call to attribute names like write_text, rmtree, etc.

Important detail Statically, you often can‚Äôt know the real path. Use a rule like:

allow literal paths only (strict mode), OR

require that file paths originate from an injected safe helper like ctx.output_path("x.json")

7) Cap output / logging flood (print storms)

What to catch

giant print() inside loops

while True with printing

writing huge strings to stdout

Why This is the common ‚Äúagent stalls your TUI and eats memory‚Äù failure mode.

AST target

ast.While with test True

ast.For / ast.While containing print() or sys.stdout.write()

Heuristic Flag if:

print appears inside any loop AND no obvious bounded range (range(N) where N is a small constant) is detectable.

8) Kill the classic infinite loop / runaway recursion patterns

What to catch

while True: without a break or without a decreasing counter (heuristic)

recursion without base case (hard statically, but you can flag obvious cases)

Why Even with timeouts, you want early rejection and better error messages.

AST target

ast.While where test is ast.Constant(True)

optionally, scan body for ast.Break presence

9) Block introspection / ‚Äúdunder escape‚Äù gadgets

What to catch

Access to attributes like:

__globals__, __getattribute__, __subclasses__, __mro__, __dict__

func.__code__, co_consts, f_locals, frame usage

inspect module (often used to reach frames)

Why A lot of Python sandbox escapes pivot through object model introspection.

AST target

ast.Attribute with attr starting/ending __

imports of inspect, gc (sometimes used to find objects)

Rule of thumb If you see __subclasses__, treat it like a red alert unless you explicitly allow it.

10) Ban package installation / environment mutation

What to catch

pip invoked via subprocess, os.system

sys.path.append(...) / site.addsitedir(...) (optional but smart)

importlib.machinery / importlib.util.spec_from_file_location (often used to load arbitrary code)

writing to .pth or site-packages paths

Why This is how code makes itself ‚Äústick‚Äù or pulls in new capabilities.

AST target

calls touching sys.path, site.*, importlib.* loaders

plus your process execution checks

The practical ‚Äúprofile‚Äù approach (how this stays sane)

Instead of one mega-policy, define profiles:

SAFE_EVAL: only pure computation + formatting

No filesystem writes

No network

Tiny import allowlist

DATA_PROCESSING: allow /input reads and /output writes

allow numpy/pandas

still no network, no subprocess

PLOTTING: allow matplotlib and write images to /output

REPO_TOOLING: allow limited filesystem within a workspace mount (still no shell by default)

Each profile = allowlist imports + allowed IO helpers + limits.

One power move that makes these checks way stronger

Inject a ctx object and require all side effects go through it:

ctx.read_text("input.csv")

ctx.write_json("result.json", data)

ctx.log("...")

Then your AST checks can be stricter:

ban raw open()

ban print()

ban direct Path.write_*

That massively reduces risk and increases observability.

If you want, I can turn these into:

a rule table (RuleID, severity, node types, examples, message), and

a minimal AST checker skeleton (built-in ast) that supports profiles + alias tracking + clear error reporting for Lily‚Äôs control-room UI.
